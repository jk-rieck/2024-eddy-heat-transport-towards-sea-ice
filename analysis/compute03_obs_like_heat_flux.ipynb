{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy import ndimage\n",
    "from scipy.spatial import ConvexHull\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cftime\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a951c0-6dc1-46f0-9df7-0b0585dae681",
   "metadata": {},
   "source": [
    "## Compute heat transport like from observations\n",
    "We co-locate eddies from tracks with regions in eddymask. Then we compute heat content as a sum over all the T' in one connected region and relate this to the translational eddy velocity computed from the latitudinal displacement of the eddy between `t-1` and `t+1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fe8ca-c614-4f25-9144-2dd36bf4a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/path/to/model/output/\" \n",
    "eddypath = \"/path/to/tracked/eddies/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d980f-0232-4b5e-8719-7ce98eb9d797",
   "metadata": {},
   "source": [
    "We run the calculations in parallel with `multiprocessing`, here we define how many cpus to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35ab216-ca24-496b-aafa-3b9da1879769",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpu = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666aa6e-9c0a-447d-bffc-a89506e5b651",
   "metadata": {},
   "source": [
    "Define a function to get indeces of the regions detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd7357-3299-4b02-9598-cce81e224524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indeces(data):\n",
    "    d = data.ravel()\n",
    "    f = lambda x: np.unravel_index(x.index, data.shape)\n",
    "    return pd.Series(d).groupby(d).apply(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a93b9-9ea3-4025-b214-8490597a71e6",
   "metadata": {},
   "source": [
    "Define the function that computes the eddy velocities as described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6436e-1061-4634-85fb-a6f5d94c730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_eddy_vel(r):\n",
    "    d = dummy.copy()\n",
    "    # loop over the assigned tracks\n",
    "    for rt in rangeTracks[r]:\n",
    "        track = tracks[rt]\n",
    "        lenTrack = len(track[\"time\"])\n",
    "        # for each track, compute velocity at each time\n",
    "        for time in np.arange(0, lenTrack):\n",
    "            with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                # get the latitude and longitude of the eddy\n",
    "                lon = np.mean(track[\"eddy_i\"][time])\n",
    "                lat = np.mean(track[\"eddy_j\"][time])\n",
    "                # load the eddymask of this time step\n",
    "                if ((track[\"time\"][time] <= lastTime)\n",
    "                    & (track[\"time\"][time] >= firstTime)):\n",
    "                    emt = em.sel(time=track[\"time\"][time]).copy()\n",
    "                else:\n",
    "                    break\n",
    "                # define connected regions\n",
    "                regions, nregions = ndimage.label((emt > 0.).astype(int))\n",
    "                region_index = get_indeces(regions)\n",
    "                point = np.array([lon, lat])\n",
    "                i = 0\n",
    "                is_inside = False\n",
    "                # check which region the current eddy is associated with\n",
    "                while not is_inside: \n",
    "                    if i < (nregions - 1):\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        break\n",
    "                    p = np.array([region_index[i][1], region_index[i][0]]).T\n",
    "                    try:\n",
    "                        hullp = ConvexHull(p)\n",
    "                        poly = plt.Polygon(p[hullp.vertices, :])\n",
    "                        is_inside = poly.contains_point(point)\n",
    "                    except:\n",
    "                        continue\n",
    "                # if the region is found, compute the velocity and multiply the mask by it\n",
    "                if is_inside:\n",
    "                    # the first and last velocities are only based on the first and last time\n",
    "                    if time == 0:\n",
    "                        lat1 = track[\"lat\"][time]\n",
    "                        lat2 = track[\"lat\"][time + 1]\n",
    "                        vobs = (lat2 - lat1) / (3600 * 24 * 5)\n",
    "                    elif ((time > 0) \n",
    "                          & (time < (lenTrack - 1))):\n",
    "                        lat1 = track[\"lat\"][time - 1]\n",
    "                        lat2 = track[\"lat\"][time + 1]\n",
    "                        vobs = (lat2 - lat1) / (3600 * 24 * 10)\n",
    "                    elif time == (lenTrack - 1):\n",
    "                        lat1 = track[\"lat\"][time - 1]\n",
    "                        lat2 = track[\"lat\"][time]\n",
    "                        vobs = (lat2 - lat1) / (3600 * 24 * 5)\n",
    "                    # use the correct time (corrsponding to the on in d)\n",
    "                    if ((track[\"time\"][time] <= lastTime)\n",
    "                        & (track[\"time\"][time] >= firstTime)):\n",
    "                        realTime = (d.time == track[\"time\"][time]).argmax()\n",
    "                        d[realTime,:,:] = (d[realTime,:,:] * vobs).where(regions == i, \n",
    "                                                                           other=d[realTime,:,:])\n",
    "                    else:\n",
    "                        break\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f6aa3-3ee9-4bca-b25a-25f5caaa65f0",
   "metadata": {},
   "source": [
    "Now we do the calculation of velocities for each year, multiply by $\\rho$, $C_{p}$ and $T'$ to get the heat transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab9335-47fc-4388-a057-ff6785d0f08a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open the eddy tracks\n",
    "with open(eddypath + 'tracks_02010101_03001230.pickle', 'rb') as f:\n",
    "    tracks = pickle.load(f)\n",
    "\n",
    "f.close()\n",
    "# open the data interpolated to F-points (eddies are defined on F-points)\n",
    "data_int = xr.open_mfdataset(eddypath + 'interp_data.extend800.0*.nc', \n",
    "                             concat_dim=\"time\", combine=\"nested\",\n",
    "                             data_vars='minimal', coords='minimal', \n",
    "                             compat='override').isel(lon=slice(0, 240))\n",
    "# load the eddy mask\n",
    "eddymask = xr.open_mfdataset(eddypath + 'eddymask_binary_0201-0300_k15.new0.3.all.nc', \n",
    "                             concat_dim=\"time\", combine=\"nested\",\n",
    "                             data_vars='minimal', coords='minimal', \n",
    "                             compat='override').eddymask_binary.mean(\"z\")\n",
    "rho = 1035.\n",
    "Cp = 3994.\n",
    "t1 = np.arange(201, 300, 10)\n",
    "t2 = np.arange(210, 301, 10)\n",
    "# loop over decades\n",
    "for a, b in zip(t1, t2):\n",
    "    y1 = \"0\" + str(a)\n",
    "    y2 = \"0\" + str(b)  \n",
    "    # compute T' for the decade\n",
    "    di = data_int.sel(time=slice(y1 + '-01-01', y2 + '-12-30'))\n",
    "    tbar = di.THETA.groupby('time.month').mean(\"time\")\n",
    "    tprime = di.THETA.groupby('time.month') - tbar\n",
    "    tprime = tprime.compute()\n",
    "    # loop over years to calculate translational velocities\n",
    "    for y in np.arange(a, b+1):\n",
    "        result = 0\n",
    "        Y = \"0\" + str(y)\n",
    "        # select variables and timestamp for this year\n",
    "        tp = tprime.sel(time=slice(Y + '-01-01', Y + '-12-30')).compute()\n",
    "        savetime = tp.time.isel(time=slice(int(len(tp.time) / 2), int(len(tp.time) / 2) + 1))\n",
    "        em = (eddymask.sel(time=slice(Y + '-01-01', Y + '-12-30')) * (tp / tp)).compute()\n",
    "        # create dummy to write into\n",
    "        dummy = em.copy()\n",
    "        # detect first and last timestamps\n",
    "        firstTime = dummy.time[0].values\n",
    "        lastTime = dummy.time[-1].values\n",
    "        # find out which tracks correspond to these timestamps\n",
    "        startTrack = 0\n",
    "        while tracks[startTrack][\"time\"][-1] < firstTime:\n",
    "            startTrack += 1   \n",
    "        lastTrack = 0\n",
    "        while ((tracks[lastTrack][\"time\"][0] < lastTime)\n",
    "               & (lastTrack < len(tracks) - 1)):\n",
    "            lastTrack += 1  \n",
    "        lastTrack -= 1\n",
    "        rT = np.arange(startTrack, lastTrack)\n",
    "        # divide the range of tracks into equally long segments\n",
    "        lenSeg = np.floor(len(rT) / ncpu).astype(int)\n",
    "        rangeTracks = {}\n",
    "        for n in np.arange(0, ncpu):\n",
    "            start = n * (lenSeg)\n",
    "            end = (n + 1) * lenSeg\n",
    "            if n == ncpu - 1:\n",
    "                end = lastTrack\n",
    "            rangeTracks[n] = rT[start:end]\n",
    "        # compute eddy velocity in parallel for each segment\n",
    "        with mp.Pool(ncpu) as pool:\n",
    "            result = pool.map(calc_eddy_vel, np.arange(0, len(rangeTracks)))\n",
    "        # comine the results into one array\n",
    "        final = 0\n",
    "        for i in np.arange(0, len(rangeTracks)):\n",
    "            final += result[i].where(result[i]!=1, other=0)\n",
    "        eddyV = final.compute()\n",
    "        # compute THT from eddyV and T' and write to disk\n",
    "        THTobs = xr.DataArray(\n",
    "            (rho * Cp * (eddyV * tp) * di.dxV).sum(\"lon\").mean(\"time\").values[None, :, :], \n",
    "            coords={\"time\": savetime, \"z\": tp.z, \"lat\": tp.lat},\n",
    "            dims=[\"time\", \"lat\", \"z\"], name='THTobs')\n",
    "        THTobs.to_netcdf(path + \"post/THTobs.\" + Y + \".nc\")\n",
    "        for s in [\"DJF\", \"MAM\", \"JJA\", \"SON\"]:\n",
    "            THTobsSeason = xr.DataArray(\n",
    "                (rho * Cp * (eddyV * tp) \n",
    "                 * di.dxV).sum(\"lon\").groupby(\"time.season\").mean(\"time\").sel(season=s).values[None, :, :],\n",
    "                coords={\"time\": savetime, \"z\": tp.z, \"lat\": tp.lat},\n",
    "                dims=[\"time\", \"lat\", \"z\"], name='THTobs')\n",
    "            THTobsSeason.to_netcdf(path + \"post/THTobs.\" + s + \".\" + Y + \".nc\")\n",
    "        # create trackmask (equivalent to eddymask but only for tracked eddies)\n",
    "        tm = eddyV.where(eddyV==0, other=1).isel(z=15)\n",
    "        tm.rename(\"trackmask\").to_netcdf(eddypath + \"trackmask_\" + Y + \".nc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b6941-b14c-4824-b337-33a84211188d",
   "metadata": {},
   "source": [
    "### repeat for anticyclones and cyclones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python eddytools",
   "language": "python",
   "name": "py_eddytools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

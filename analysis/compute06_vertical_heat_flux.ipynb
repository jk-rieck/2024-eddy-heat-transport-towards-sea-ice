{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xgcm\n",
    "import seawater as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0869c47-58bf-4e6b-94d8-0ae7dbeb3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/path/to/model/output/\" \n",
    "eddypath = \"/path/to/tracked/eddies/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb2d9b-af74-4669-bc62-8ec2336e6ed3",
   "metadata": {},
   "source": [
    "### Compute vertical heat flux\n",
    "This is done analogously to the horizontal HTs with $w$, $\\langle{w}\\rangle$ and $w'$ replacing the total, mean and transient horizontal counterparts, respectively.\n",
    "\n",
    "First, we define some functions to compute the $'$-quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e28507-fd30-448d-9464-a143056dc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_prime(var):\n",
    "    varbar = var.groupby('time.month').mean(\"time\")\n",
    "    varprime = (var.groupby('time.month') - varbar)\n",
    "    return varbar, varprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab4f10b-23c5-4f39-87c1-6a3b6aa28645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_prime_season(var, season):\n",
    "    varbar = var.groupby('time.month').mean(\"time\")\n",
    "    varbar_season = var.groupby('time.season').mean(\"time\").sel(season=season)\n",
    "    varprime = (var.groupby('time.month') - varbar)\n",
    "    return varbar_season, varprime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c983da-3fc1-4a4c-8b8b-aef774d452f2",
   "metadata": {},
   "source": [
    "Define constants and load data. We also set up the grid for interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3bc0b9-73be-4624-b494-9cab16eb20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 1035.\n",
    "Cp = 3994."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccfe72c-93b9-4e31-977d-4c0e42e9a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5d = xr.open_zarr(path + \"zarr_Diags/output.5d.zarr\").sel(time=slice(\"0201-01-01\", \"0300-12-30\"))\n",
    "\n",
    "metrics = {\n",
    "    ('X'): ['dxC', 'dxG', 'dxF', 'dxV'], # X distances\n",
    "    ('Y'): ['dyC', 'dyG', 'dyF', 'dyU'], # Y distances\n",
    "    ('Z'): ['drF', 'drW', 'drS', 'drC'], # Z distances\n",
    "    ('X', 'Y'): ['rAw', 'rAs', 'rA', 'rAz'] # Areas in x-y plane\n",
    "}\n",
    "\n",
    "grid = xgcm.Grid(data5d, periodic=[\"X\"], metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567bb57d-7e83-4979-bc1c-e09e49fb23d6",
   "metadata": {},
   "source": [
    "Eddymasks are on F-point due to detection method so need to make sure to interpolate other things to F-point later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5576f9-409a-4607-a714-7c9faa425ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eddymask = xr.open_mfdataset(eddypath + 'eddymask_0201-0300.nc', concat_dim=\"time\", combine=\"nested\",\n",
    "                             data_vars='minimal', coords='minimal', \n",
    "                             compat='override').eddymask.squeeze().rename({\"lon\": \"XG\", \"lat\": \"YG\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5279d48-1483-428c-b568-5b38b972f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eddymask_cyclones = xr.open_mfdataset(eddypath + 'eddymask_cyclones_0201-0300.nc', concat_dim=\"time\", combine=\"nested\",\n",
    "                                      data_vars='minimal', coords='minimal', \n",
    "                                      compat='override').eddymask_cyclones.squeeze().rename({\"lon\": \"XG\", \"lat\": \"YG\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de67ace-66a3-4b69-8712-6013f4302ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eddymask_anticyclones = xr.open_mfdataset(eddypath + 'eddymask_anticyclones_0201-0300.nc', concat_dim=\"time\", combine=\"nested\",\n",
    "                                          data_vars='minimal', coords='minimal', \n",
    "                                          compat='override').eddymask_anticyclones.squeeze().rename({\"lon\": \"XG\", \"lat\": \"YG\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca5698-7fe2-4453-9a8b-d91f1f5f2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons = [\"DJF\", \"MAM\", \"JJA\", \"SON\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b16d5-814f-4d5b-80b8-624a28c63762",
   "metadata": {},
   "source": [
    "Loop over the 10 decades and compute the mean HT for the whole year and the four seasons within each decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f6f38-a81d-40d8-84ed-6d58324a18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    print(i)\n",
    "    # extract the desired decade\n",
    "    data = data5d.isel(time=slice(720 * i, 720 * (i+1)))\n",
    "    y1 = str(data.time.isel(time=0).dt.year.values)\n",
    "    y2 = str(data.time.isel(time=-1).dt.year.values)\n",
    "    # this is the timestamp used in the saved files later on\n",
    "    savetime = data.time.isel(time=slice(int(len(data.time) / 2), int(len(data.time) / 2) + 1))\n",
    "    # compute the reference temperature\n",
    "    tref = sw.eos80.fp(data.SALT, 1.065)\n",
    "    # define the mean winter mixed layer depth, the depth across which we compute VHT\n",
    "    meanJJAMLD = grid.interp(data.MXLDEPTH.groupby(\"time.season\").mean(\"time\").sel(season=\"JJA\"), (\"X\", \"Y\"), boundary=\"extend\").compute()\n",
    "    SIarea = grid.interp(data.SIarea, (\"X\", \"Y\"), boundary=\"extend\")\n",
    "    # need to interpolate THETA to the W-grid\n",
    "    w = data.WVEL\n",
    "    TonW = grid.interp(data.THETA - tref, \"Z\", boundary=\"extend\")\n",
    "    T = TonW\n",
    "    # compute mean and anomalies\n",
    "    wbar, wprime = bar_prime(w)\n",
    "    Tbar, Tprime = bar_prime(T)\n",
    "    # compute heat transports (we multiply already by dx because we will integrate in x-direction later)\n",
    "    # and we interpolate to F-points because that's were the eddies are defined\n",
    "    VHTtotal = (grid.interp(rho * Cp * w * T, (\"Y\", \"X\"), boundary=\"extend\") * data.dxV)\n",
    "    VHTprime = (grid.interp(rho * Cp * wprime * Tprime, (\"Y\", \"X\"), boundary=\"extend\") * data.dxV)\n",
    "    VHTbar = (grid.interp(rho * Cp * wbar * Tbar, (\"Y\", \"X\"), boundary=\"extend\") * data.dxV)\n",
    "    # contribution by CME\n",
    "    VHTprime_eddy = VHTprime * eddymask\n",
    "    VHTprime_eddy_anticyclones = VHTprime * eddymask_anticyclones\n",
    "    VHTprime_eddy_cyclones = VHTprime * eddymask_cyclones\n",
    "    ## same for diffusive part\n",
    "    DIFF = data.DFrI_TH\n",
    "    DIFFbar, DIFFprime = bar_prime(DIFF)\n",
    "    VHTDIFFtotal = grid.interp(rho * Cp * DIFF, (\"X\", \"Y\"), boundary=\"extend\")\n",
    "    VHTDIFFprime = grid.interp(rho * Cp * DIFFprime, (\"X\", \"Y\"), boundary=\"extend\")\n",
    "    VHTDIFFbar = grid.interp(rho * Cp * DIFFbar, (\"X\", \"Y\"), boundary=\"extend\")\n",
    "    VHTDIFFprime_eddy = VHTDIFFprime * eddymask\n",
    "    VHTDIFFprime_eddy_anticyclones = VHTDIFFprime * eddymask_anticyclones\n",
    "    VHTDIFFprime_eddy_cyclones = VHTDIFFprime * eddymask_cyclones\n",
    "    # select heat transports at the base of the winter ML \n",
    "    VHTtotalMLD = VHTtotal.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    VHTprimeMLD = VHTprime.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    VHTbarMLD = VHTbar.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    VHTprimeMLD_eddy = VHTprime_eddy.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    VHTprimeMLD_eddy_cyclones = VHTprime_eddy_cyclones.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    VHTprimeMLD_eddy_anticyclones = VHTprime_eddy_anticyclones.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    VHTDIFFtotalMLD = VHTDIFFtotal.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    VHTDIFFprimeMLD = VHTDIFFprime.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    VHTDIFFbarMLD = VHTDIFFbar.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    VHTDIFFprimeMLD_eddy = VHTDIFFprime_eddy.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    VHTDIFFprimeMLD_eddy_cyclones = VHTDIFFprime_eddy_cyclones.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    VHTDIFFprimeMLD_eddy_anticyclones = VHTDIFFprime_eddy_anticyclones.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "    # zonal integrals (already multiplied by dx above)\n",
    "    VHTtotalMLDX = VHTtotalMLD.sum(\"XG\")\n",
    "    VHTprimeMLDX = VHTprimeMLD.sum(\"XG\")\n",
    "    VHTbarMLDX = VHTbarMLD.sum(\"XG\")\n",
    "    VHTprimeMLD_eddyX = VHTprimeMLD_eddy.sum(\"XG\")\n",
    "    VHTprimeMLD_eddy_cyclonesX = VHTprimeMLD_eddy_cyclones.sum(\"XG\")\n",
    "    VHTprimeMLD_eddy_anticyclonesX = VHTprimeMLD_eddy_anticyclones.sum(\"XG\")\n",
    "    VHTDIFFtotalMLDX = VHTDIFFtotalMLD.sum(\"XG\")\n",
    "    VHTDIFFprimeMLDX = VHTDIFFprimeMLD.sum(\"XG\")\n",
    "    VHTDIFFbarMLDX = VHTDIFFbarMLD.sum(\"XG\")\n",
    "    VHTDIFFprimeMLD_eddyX = VHTDIFFprimeMLD_eddy.sum(\"XG\")\n",
    "    VHTDIFFprimeMLD_eddy_cyclonesX = VHTDIFFprimeMLD_eddy_cyclones.sum(\"XG\")\n",
    "    VHTDIFFprimeMLD_eddy_anticyclonesX = VHTDIFFprimeMLD_eddy_anticyclones.sum(\"XG\")\n",
    "    # create a dataset and write all the different terms into it (time-averaged over the current decade)\n",
    "    VHT_across_MLD = xr.Dataset(coords={\"time\": savetime, \"YG\": data.YG.values})\n",
    "    VHT_across_MLD[\"VHTtotal\"] = xr.DataArray(VHTtotalMLDX.mean(\"time\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    VHT_across_MLD[\"VHTprime\"] = xr.DataArray(VHTprimeMLDX.mean(\"time\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    VHT_across_MLD[\"VHTbar\"] = xr.DataArray(VHTbarMLDX.mean(\"month\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    VHT_across_MLD[\"VHTeddy\"] = xr.DataArray(VHTprimeMLD_eddyX.mean(\"time\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    VHT_across_MLD[\"VHTeddy_cyclones\"] = xr.DataArray(VHTprimeMLD_eddy_cyclonesX.mean(\"time\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    VHT_across_MLD[\"VHTeddy_anticyclones\"] = xr.DataArray(VHTprimeMLD_eddy_anticyclonesX.mean(\"time\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    VHT_across_MLD[\"VHTDIFFtotal\"] = xr.DataArray(VHTDIFFtotalMLDX.mean(\"time\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    VHT_across_MLD[\"VHTDIFFprime\"] = xr.DataArray(VHTDIFFprimeMLDX.mean(\"time\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    VHT_across_MLD[\"VHTDIFFbar\"] = xr.DataArray(VHTDIFFbarMLDX.mean(\"month\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    VHT_across_MLD[\"VHTDIFFeddy\"] = xr.DataArray(VHTDIFFprimeMLD_eddyX.mean(\"time\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    VHT_across_MLD[\"VHTDIFFeddy_cyclones\"] = xr.DataArray(VHTDIFFprimeMLD_eddy_cyclonesX.mean(\"time\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    VHT_across_MLD[\"VHTDIFFeddy_anticyclones\"] = xr.DataArray(VHTDIFFprimeMLD_eddy_anticyclonesX.mean(\"time\").values[None, :], dims=(\"time\", \"YG\"))\n",
    "    # also add the mixed layer depth used to compute the HT to the dataset\n",
    "    VHT_across_MLD[\"MLD\"] = meanJJAMLD.mean(\"XG\")\n",
    "    # write dataset to disk\n",
    "    VHT_across_MLD.to_netcdf(path + \"post/VHT_across_base_of_winter_MLD.new0.3.0\" + y1 + \"_0\" + y2 + \".all.no_int.nc\")\n",
    "    # repeat everything for the different seasons\n",
    "    for season in seasons:\n",
    "        print(season)\n",
    "        wbar, _ = bar_prime_season(w, season)\n",
    "        Tbar, _ = bar_prime_season(T, season)\n",
    "        DIFFbar, _ = bar_prime_season(DIFF, season)\n",
    "        VHTbar = (grid.interp(rho * Cp * wbar * Tbar, (\"Y\", \"X\"), boundary=\"extend\") * data.dxV).compute()\n",
    "        VHTDIFFbar = grid.interp(rho * Cp * DIFFbar, (\"X\", \"Y\"), boundary=\"extend\").compute()\n",
    "        VHTbarMLD = VHTbar.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "        VHTDIFFbarMLD = VHTDIFFbar.sel(Zl=-meanJJAMLD, method=\"ffill\")\n",
    "        VHTbarMLDX = VHTbarMLD.sum(\"XG\")\n",
    "        VHTDIFFbarMLDX = VHTDIFFbarMLD.sum(\"XG\")\n",
    "        #\n",
    "        VHT_across_MLD = xr.Dataset(coords={\"time\": savetime, \"YG\": data.YG.values})\n",
    "        VHT_across_MLD[\"VHTprime\"] =\\\n",
    "            xr.DataArray(VHTprimeMLDX.groupby(\"time.season\").mean(\"time\").sel(season=season).values[None, :], dims=(\"time\", \"YG\"))\n",
    "        VHT_across_MLD[\"VHTtotal\"] =\\\n",
    "            xr.DataArray(VHTtotalMLDX.groupby(\"time.season\").mean(\"time\").sel(season=season).values[None, :], dims=(\"time\", \"YG\"))\n",
    "        VHT_across_MLD[\"VHTbar\"] =\\\n",
    "            xr.DataArray(VHTbarMLDX.values[None, :], dims=(\"time\", \"YG\"))\n",
    "        VHT_across_MLD[\"VHTeddy\"] =\\\n",
    "            xr.DataArray(VHTprimeMLD_eddyX.groupby(\"time.season\").mean(\"time\").sel(season=season).values[None, :], dims=(\"time\", \"YG\"))\n",
    "        VHT_across_MLD[\"VHTeddy_cyclones\"] =\\\n",
    "            xr.DataArray(VHTprimeMLD_eddy_cyclonesX.groupby(\"time.season\").mean(\"time\").sel(season=season).values[None, :], dims=(\"time\", \"YG\"))\n",
    "        VHT_across_MLD[\"VHTeddy_anticyclones\"] =\\\n",
    "            xr.DataArray(VHTprimeMLD_eddy_anticyclonesX.groupby(\"time.season\").mean(\"time\").sel(season=season).values[None, :], dims=(\"time\", \"YG\"))\n",
    "        VHT_across_MLD[\"VHTDIFFprime\"] =\\\n",
    "            xr.DataArray(VHTDIFFprimeMLDX.groupby(\"time.season\").mean(\"time\").sel(season=season).values[None, :], dims=(\"time\", \"YG\"))\n",
    "        VHT_across_MLD[\"VHTDIFFbar\"] =\\\n",
    "            xr.DataArray(VHTDIFFbarMLDX.values[None, :], dims=(\"time\", \"YG\"))\n",
    "        VHT_across_MLD[\"VHTDIFFeddy\"] =\\\n",
    "            xr.DataArray(VHTDIFFprimeMLD_eddyX.groupby(\"time.season\").mean(\"time\").sel(season=season).values[None, :], dims=(\"time\", \"YG\"))\n",
    "        VHT_across_MLD[\"VHTDIFFeddy_cyclones\"] =\\\n",
    "            xr.DataArray(VHTDIFFprimeMLD_eddy_cyclonesX.groupby(\"time.season\").mean(\"time\").sel(season=season).values[None, :], dims=(\"time\", \"YG\"))\n",
    "        VHT_across_MLD[\"VHTDIFFeddy_anticyclones\"] =\\\n",
    "            xr.DataArray(VHTDIFFprimeMLD_eddy_anticyclonesX.groupby(\"time.season\").mean(\"time\").sel(season=season).values[None, :], dims=(\"time\", \"YG\"))\n",
    "        #\n",
    "        VHT_across_MLD[\"MLD\"] = meanJJAMLD.mean(\"XG\")\n",
    "        VHT_across_MLD.to_netcdf(path + \"post/VHT_across_base_of_winter_MLD.\" + season + \"0\" + y1 + \"_0\" + y2 + \".nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1901f-95f3-44cb-a8f8-006b1a81fa80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python eddytools",
   "language": "python",
   "name": "py_eddytools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import seawater as sw\n",
    "import xgcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/path/to/model/output/\" \n",
    "eddypath = \"/path/to/tracked/eddies/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b345e-8520-4cf5-b1d5-820e3b1cfb60",
   "metadata": {},
   "source": [
    "## Calculate the different forms of heat transports on z-levels\n",
    "\n",
    "The zonally integrated, time-averaged total HT is defined as  \n",
    "\n",
    "$HT(y, z) = \\rho C_{p} [\\overline{v\\, (T - T_{f})}]$,  \n",
    "\n",
    "where $\\rho = 1035$ kg$\\,$m$^{-3}$ is the model's reference density, $C_{p} = 3994$ J$\\,$kg$^{-1}\\,$K$^{-1}$ is the specific heat capacity of seawater at constant pressure, $v$ is meridional velocity, $T$ is the model's potential temperature and $T_{f}$ is the freezing point temperature. The use of $T_{f}$ ensures that HT remains positive, whenever the meridional velocity is positive. The square brackets $[\\cdot]$ indicate zonal integration over the entire domain ($\\oint\\,\\,dx$) and the overbar the decadal mean. \n",
    "\n",
    "The mean HT, $MHT$, is based on a monthly climatology computed over each decade as $MHT(y, z) = \\rho C_{p} [\\overline{\\langle v\\, \\rangle \\langle (T - T_{f}) \\rangle}]$, where $\\langle \\cdot \\rangle$ indicates the monthly climatology. This allows to restrict the calculations to single seasons if desired, *e.g.* $MHT_{JJA}(y, z) = \\rho C_{p} [\\overline{\\langle v\\, \\rangle \\langle (T - T_{f}) \\rangle}^{JJA}]$, where $\\overline{\\cdot}^{JJA}$ represents the decadal mean over June-July-August. The largest contribution to MHT is expected to be that of the surface Ekman transport which can be computed as $MHT^{Ek}(y) = \\rho C_{p} [\\overline{\\langle v^{Ek}\\, \\rangle \\langle (T^{Ek} - T_{f}) \\rangle}]$, where $T^{Ek}$ is the temperature averaged over the surface Ekman layer (assumed to be $50$ m deep) and $v^{Ek} = \\frac{\\tau_{x}}{\\rho\\,f}$ is the meridional Ekman transport with $\\tau_{x}$ being the zonal wind stress and $f$ the Coriolis parameter.\n",
    "\n",
    "The transient part of the meridional HT is calculated as  \n",
    "\n",
    "$THT(y, z) = \\rho C_{p} [\\overline{v'T'}]$,  \n",
    "\n",
    "where primes indicate the deviation from the monthly climatology such that $v' = v - \\langle v \\rangle$ and $T' = T - \\langle T \\rangle$. \n",
    "\n",
    "The contribution of coherent eddies to transient heat transport ($THT$) is considered by multiplying $THT$ with the mask $M^{CME}$ before integrating. In this way only the $THT$ that occurs within coherent mesoscale eddies contributes to the integrals. \n",
    "\n",
    "$THT^{CME}(y, z) = \\rho C_{p} [\\overline{v'T'\\,M^{CME}}]$\n",
    "\n",
    "First, a function to load the model output and eddymasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509904ce-9201-4199-acaa-e8e3dd0d20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path, eddypath, y1, y2):\n",
    "    data = xr.open_zarr(path + \"zarr_Diags/output.5d.zarr\").sel(time=slice(y1 + \"-01-01\", \n",
    "                                                                           y2 + \"-12-30\"))\n",
    "    THTdiv = xr.open_mfdataset(path + \"post/THTvdiv.0???.nc\").sel(time=slice(y1 + \"-01-01\", \n",
    "                                                                             y2 + \"-12-30\"))\n",
    "    em = xr.open_mfdataset(eddypath\n",
    "        + 'eddymask_0201-0300.nc').eddymask_binary.squeeze().rename({\"lon\": \"XG\", \"lat\": \"YG\"})\n",
    "    em_cyclones = xr.open_mfdataset(eddypath \n",
    "        + 'eddymask_cyclones_0201-0300.nc').eddymask_cyclones_binary.squeeze().rename({\"lon\": \"XG\", \"lat\": \"YG\"})\n",
    "    em_anticyclones = xr.open_mfdataset(eddypath \n",
    "        + 'eddymask_anticyclones_0201-0300.nc').eddymask_anticyclones_binary.squeeze().rename({\"lon\": \"XG\", \"lat\": \"YG\"})\n",
    "    tm = xr.open_mfdataset(eddypath \n",
    "        + 'trackmask_0201-0300.nc').trackmask_binary.squeeze().rename({\"lon\": \"XG\", \"lat\": \"YG\"})\n",
    "    tm_cyclones = xr.open_mfdataset(eddypath \n",
    "        + 'trackmask_cyclones_0201-0300.nc').trackmask_cyclones_binary.squeeze().rename({\"lon\": \"XG\", \"lat\": \"YG\"})\n",
    "    tm_anticyclones = xr.open_mfdataset(eddypath \n",
    "        + 'trackmask_anticyclones_0201-0300.nc').trackmask_anticyclones_binary.squeeze().rename({\"lon\": \"XG\", \"lat\": \"YG\"})\n",
    "    return data, THTdiv, em, em_cyclones, em_anticyclones, tm, tm_cyclones, tm_anticyclones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ce7c4-366b-4d48-8387-038247a4b679",
   "metadata": {},
   "source": [
    "A function to create the `xgcm.Grid()` instance for interpolation, set constants needed for the calculation of heat transport and prepare some masks to exclude the shelf (`maskShelf`) and select only the bottom grid cells (`bottom`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b4275-ebdc-43b4-9f3a-5d65c101f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_and_prepare(data, THTdiv):\n",
    "    # create `grid`\n",
    "    metrics = {\n",
    "        ('X'): ['dxC', 'dxG', 'dxF', 'dxV'], # X distances\n",
    "        ('Y'): ['dyC', 'dyG', 'dyF', 'dyU'], # Y distances\n",
    "        ('Z'): ['drF', 'drW', 'drS', 'drC'], # Z distances\n",
    "        ('X', 'Y'): ['rAw', 'rAs', 'rA', 'rAz'] # Areas in x-y plane\n",
    "        }\n",
    "    grid = xgcm.Grid(data, periodic=[\"X\"], metrics=metrics)\n",
    "    # set constants\n",
    "    rho = 1035. # reference density\n",
    "    Cp = 3994. # heat capacity\n",
    "    f0 = -1.405e-4 # reference Coriolis\n",
    "    beta = 1.145e-11 # beta\n",
    "    Cd = 5.0e-2 # drag coefficient for calculation of bottom Ekman transport\n",
    "    f = f0 + beta * data.YC\n",
    "    # reference temperature is freezing point temperature with \n",
    "    # local salinity\n",
    "    tref = sw.eos80.fp(data.SALT, 1.065) \n",
    "    # create a timestamp for saving the heat transport to disk\n",
    "    savetime = data.time.isel(time=slice(int(len(data.time) / 2), int(len(data.time) / 2) + 1))\n",
    "    # ceate masks\n",
    "    dMask = data.maskS * data.Z\n",
    "    bottom = dMask.min(\"Z\")\n",
    "    tmpDepth = dMask.where(dMask > dMask.min(\"Z\"), other=0).min(\"Z\")\n",
    "    nearBottom = dMask.where(dMask > tmpDepth, other=0).min(\"Z\")\n",
    "    maskShelf = np.ones(np.shape(THTdiv.THTvdiv.isel(time=0)))\n",
    "    for k in np.arange(0, np.shape(maskShelf)[0]):\n",
    "        firstwet = np.argmax(THTdiv.THTvdiv[0, k, :, 0].values != 0.)\n",
    "        maskShelf[k, 0:firstwet+1, :] = 0\n",
    "    data[\"maskShelf\"] = xr.DataArray(maskShelf, dims=[\"Z\", \"YG\", \"XC\"])  \n",
    "    # prepare temperature and velocity\n",
    "    T = data.THETA - tref\n",
    "    V = data.VVEL * data.hFacS\n",
    "    shelf = data.maskShelf\n",
    "    return data, T, V, shelf, rho, Cp, f0, beta, Cd, f, savetime, dMask, nearBottom, grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1e0b4-bf3f-4b30-b48e-d7aab6aa0e48",
   "metadata": {},
   "source": [
    "Now the function to calculate the mean heat transports (averaged over all months)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b7d3c-83e8-4726-bd55-20fea49b2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hts(data, THTdiv, T, V, shelf, \n",
    "             em, em_cycl, em_anti, tm, tm_cycl, tm_anti, nBotDep, grid,\n",
    "             ML, mld, mldm, mlds, mld_name):\n",
    "    # calculate mean seasonal cycle and deviation (prime)\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "        vbar = V.groupby('time.month').mean(\"time\")\n",
    "        tbar = T.groupby('time.month').mean(\"time\")\n",
    "        vprime = V.groupby('time.month') - vbar\n",
    "        tprime = T.groupby('time.month') - tbar\n",
    "    # estimate temperature in Ekman layer (averaged over the top 50 m)   \n",
    "    tauXbar = data.oceTAUX.groupby('time.month').mean(\"time\")\n",
    "    tEKmean = ((T.sel(Z=slice(0, -50)) * data.drF.sel(Z=slice(0, -50))).sum(\"Z\") \n",
    "               / data.drF.sel(Z=slice(0, -50)).sum().values)\n",
    "    tEKbar = grid.interp(tEKmean.groupby('time.month').mean(\"time\"), \"Y\", boundary=\"extend\")\n",
    "    # calulate Ekman velocity from tau\n",
    "    vEKbar = -tauXbar / (rho * f)\n",
    "    # compute total, mean and transient\n",
    "    totalht = rho * Cp * (V * grid.interp(T, \"Y\", boundary=\"extend\"))\n",
    "    mht = rho * Cp * (vbar * grid.interp(tbar, \"Y\", boundary=\"extend\"))\n",
    "    mhtek = rho * Cp * (grid.interp(vEKbar, \"Y\", boundary=\"extend\") * grid.interp(tEKbar, \"X\"))\n",
    "    tht = rho * Cp * (vprime * grid.interp(tprime, \"Y\", boundary=\"extend\"))\n",
    "    # if desired, mask out the regions specify in mld, mldm before integration\n",
    "    if ML:\n",
    "        mht = mht * mldm\n",
    "        tht = tht * mld\n",
    "    # integrate everythin zonally and average over time\n",
    "    MHT = (grid.interp(mht, \"X\") * data.dxV).sum(\"XG\").mean('month')\n",
    "    MHTek = (mhtek * data.dxV).sum(\"XG\").mean('month')\n",
    "    THT = (grid.interp(tht, \"X\") * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    # save to disk\n",
    "    Ms = xr.Dataset({\n",
    "        \"MHT\": MHT.expand_dims({\"time\": savetime}), \n",
    "        \"MHTek\": MHTek.expand_dims({\"time\": savetime}), \n",
    "        \"THT\": THT.expand_dims({\"time\": savetime})\n",
    "        })\n",
    "    Ms.to_netcdf(eddypath + 'MHTs' + mldname + '.' + str(y1) + '0101_' + str(y2) + '1230.nc')\n",
    "    # same as above but for the individual seasons\n",
    "    for s in [\"DJF\", \"MAM\", \"JJA\", \"SON\"]:\n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "            vbar_season = V.groupby('time.season').mean(\"time\").sel(season=s)\n",
    "            tbar_season = T.groupby('time.season').mean(\"time\").sel(season=s)\n",
    "        tauXbar_season = data.oceTAUX.groupby('time.season').mean(\"time\").sel(season=s)\n",
    "        tEKbar_season = grid.interp(tEKmean.groupby('time.season').mean(\"time\").sel(season=s),\n",
    "                                    \"Y\", boundary=\"extend\")\n",
    "        vEKbar_season = -tauXbar_season / (rho * f)\n",
    "        mht_season = rho * Cp * (vbar_season * grid.interp(tbar_season, \"Y\", boundary=\"extend\"))\n",
    "        mhtek_season = rho * Cp * (grid.interp(vEKbar_season, \n",
    "                                               \"Y\", boundary=\"extend\") * grid.interp(tEKbar_season, \"X\"))\n",
    "        # if desired, mask out the regions specify in mld, mldm before integration\n",
    "        if ML:\n",
    "            mht_season = mht_season * mlds\n",
    "        MHT_season = (grid.interp(mht_season, \"X\") * data.dxV).sum(\"XG\")\n",
    "        MHTek_season = (mhtek_season * data.dxV).sum(\"XG\")\n",
    "        THT_season = (grid.interp(tht, \"X\") \n",
    "                      * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        Ms_season = xr.Dataset({\n",
    "            \"MHT\": MHT_season.expand_dims({\"time\": savetime}), \n",
    "            \"MHTek\": MHTek_season.expand_dims({\"time\": savetime}),\n",
    "            \"THT\": THT_season.expand_dims({\"time\": savetime})\n",
    "            })\n",
    "        Ms_season.to_netcdf(eddypath + 'MHTs' + mldname + '.' + s + '.' + str(y1) + '0101_' + str(y2) + '1230.nc')\n",
    "    # get reference velocity for Watts et al 2016\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
    "        vref = V.sel(Z=nBotDep, method='nearest')\n",
    "        vrefbar = vref.groupby('time.month').mean(\"time\")\n",
    "        vrefprime = vref.groupby('time.month') - vrefbar\n",
    "    # compute THT_ref and THT_div\n",
    "    thtref = rho * Cp * (vrefprime * grid.interp(tprime, \"Y\", boundary=\"extend\"))\n",
    "    thtdiv = THTdiv.THTvdiv\n",
    "    # if desired, mask out the regions specify in mld, mldm before integration\n",
    "    if ML:\n",
    "        thtref = thtref * mld\n",
    "        thtdiv = thtdiv * mld\n",
    "    # integrate everything zonally and average over time, multiply with eddymask \n",
    "    # before integration to get the CME contribution\n",
    "    THTeddy = (grid.interp(tht, \"X\") * em * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTeddy_cyclones = (grid.interp(tht, \"X\") * em_cycl * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTeddy_anticyclones = (grid.interp(tht, \"X\") * em_anti * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTtrack = (grid.interp(tht, \"X\") * tm * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTtrack_cyclones = (grid.interp(tht, \"X\") * tm_cycl * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTtrack_anticyclones = (grid.interp(tht, \"X\") * tm_anti * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTref = (grid.interp(thtref, \"X\") * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTrefeddy = (grid.interp(thtref, \"X\") * em * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTrefeddy_cyclones = (grid.interp(thtref, \"X\") * em_cycl * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTrefeddy_anticyclones = (grid.interp(thtref, \"X\") * em_anti * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTreftrack = (grid.interp(thtref, \"X\") * tm * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTreftrack_cyclones = (grid.interp(thtref, \"X\") * tm_cycl * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTreftrack_anticyclones = (grid.interp(thtref, \"X\") * tm_anti * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTdiv = (grid.interp(thtdiv, \"X\")  * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTdiveddy = (grid.interp(thtdiv, \"X\") * em * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTdiveddy_cyclones = (grid.interp(thtdiv, \"X\") * em_cycl * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTdiveddy_anticyclones = (grid.interp(thtdiv, \"X\") * em_anti * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTdivtrack = (grid.interp(thtdiv, \"X\") * tm * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTdivtrack_cyclones = (grid.interp(thtdiv, \"X\") * tm_cycl * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    THTdivtrack_anticyclones = (grid.interp(thtdiv, \"X\") * tm_anti * data.dxV).sum(\"XG\").mean(\"time\")\n",
    "    # create dataset and save to disk\n",
    "    Ts = xr.Dataset({\n",
    "        \"THT\": THT.expand_dims({\"time\": savetime}), \n",
    "        \"THTeddy\": THTeddy.expand_dims({\"time\": savetime}), \n",
    "        \"THTeddy_cyclones\": THTeddy_cyclones.expand_dims({\"time\": savetime}), \n",
    "        \"THTeddy_anticyclones\": THTeddy_anticyclones.expand_dims({\"time\": savetime}),\n",
    "        \"THTtrack\": THTtrack.expand_dims({\"time\": savetime}), \n",
    "        \"THTtrack_cyclones\": THTtrack_cyclones.expand_dims({\"time\": savetime}), \n",
    "        \"THTtrack_anticyclones\": THTtrack_anticyclones.expand_dims({\"time\": savetime}),\n",
    "        \"THTref\": THTref.expand_dims({\"time\": savetime}), \n",
    "        \"THTrefeddy\": THTrefeddy.expand_dims({\"time\": savetime}), \n",
    "        \"THTrefeddy_cyclones\": THTrefeddy_cyclones.expand_dims({\"time\": savetime}), \n",
    "        \"THTrefeddy_anticyclones\": THTrefeddy_anticyclones.expand_dims({\"time\": savetime}), \n",
    "        \"THTreftrack\": THTreftrack.expand_dims({\"time\": savetime}), \n",
    "        \"THTreftrack_cyclones\": THTreftrack_cyclones.expand_dims({\"time\": savetime}), \n",
    "        \"THTreftrack_anticyclones\": THTreftrack_anticyclones.expand_dims({\"time\": savetime}), \n",
    "        \"THTdiv\": THTdiv.expand_dims({\"time\": savetime}), \n",
    "        \"THTdiveddy\": THTdiveddy.expand_dims({\"time\": savetime}), \n",
    "        \"THTdiveddy_cyclones\": THTdiveddy_cyclones.expand_dims({\"time\": savetime}), \n",
    "        \"THTdiveddy_anticyclones\": THTdiveddy_anticyclones.expand_dims({\"time\": savetime}),\n",
    "        \"THTdivtrack\": THTdivtrack.expand_dims({\"time\": savetime}), \n",
    "        \"THTdivtrack_cyclones\": THTdivtrack_cyclones.expand_dims({\"time\": savetime}), \n",
    "        \"THTdivtrack_anticyclones\": THTdivtrack_anticyclones.expand_dims({\"time\": savetime})\n",
    "        })\n",
    "    Ts.to_netcdf(eddypath + 'THTs' + mldname + '.' + str(y1) + '0101_' + str(y2) + '1230.nc')\n",
    "    # repear everything for the four individual seasons\n",
    "    for s in [\"DJF\", \"MAM\", \"JJA\", \"SON\"]:\n",
    "        THT_season = (grid.interp(tht, \"X\") \n",
    "                      * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTeddy_season = (grid.interp(tht, \"X\") \n",
    "            * em * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTeddy_cyclones_season = (grid.interp(tht, \"X\") \n",
    "            * em_cycl * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTeddy_anticyclones_season = (grid.interp(tht, \"X\") \n",
    "            * em_anti * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTtrack_season = (grid.interp(tht, \"X\") \n",
    "            * tm * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTtrack_cyclones_season = (grid.interp(tht, \"X\") \n",
    "            * tm_cycl * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTtrack_anticyclones_season = (grid.interp(tht, \"X\") \n",
    "            * tm_anti * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTref_season = (grid.interp(thtref , \"X\")\n",
    "            * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTrefeddy_season = (grid.interp(thtref, \"X\") \n",
    "            * em * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTrefeddy_cyclones_season = (grid.interp(thtref, \"X\") \n",
    "            * em_cycl * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTrefeddy_anticyclones_season = (grid.interp(thtref, \"X\") \n",
    "            * em_anti * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTreftrack_season = (grid.interp(thtref, \"X\") \n",
    "            * tm * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTreftrack_cyclones_season = (grid.interp(thtref, \"X\") \n",
    "            * tm_cycl * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTreftrack_anticyclones_season = (grid.interp(thtref, \"X\") \n",
    "            * tm_anti * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTdiv_season = (grid.interp(thtdiv, \"X\") \n",
    "            * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTdiveddy_season = (grid.interp(thtdiv, \"X\") \n",
    "            * em * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTdiveddy_cyclones_season = (grid.interp(thtdiv, \"X\") \n",
    "            * em_cycl * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTdiveddy_anticyclones_season = (grid.interp(thtdiv, \"X\") \n",
    "            * em_anti * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTdivtrack_season = (grid.interp(thtdiv, \"X\") \n",
    "            * tm * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTdivtrack_cyclones_season = (grid.interp(thtdiv, \"X\") \n",
    "            * tm_cycl * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        THTdivtrack_anticyclones_season = (grid.interp(thtdiv, \"X\") \n",
    "            * tm_anti * data.dxV).sum(\"XG\").groupby(\"time.season\").mean(\"time\").sel(season=s)\n",
    "        Ts_season = xr.Dataset({\n",
    "            \"THT\": THT_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTeddy\": THTeddy_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTeddy_cyclones\": THTeddy_cyclones_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTeddy_anticyclones\": THTeddy_anticyclones_season.expand_dims({\"time\": savetime}),\n",
    "            \"THTtrack\": THTtrack_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTtrack_cyclones\": THTtrack_cyclones_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTtrack_anticyclones\": THTtrack_anticyclones_season.expand_dims({\"time\": savetime}),\n",
    "            \"THTref\": THTref_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTrefeddy\": THTrefeddy_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTrefeddy_cyclones\": THTrefeddy_cyclones_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTrefeddy_anticyclones\": THTrefeddy_anticyclones_season.expand_dims({\"time\": savetime}),\n",
    "            \"THTreftrack\": THTreftrack_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTreftrack_cyclones\": THTreftrack_cyclones_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTreftrack_anticyclones\": THTreftrack_anticyclones_season.expand_dims({\"time\": savetime}),\n",
    "            \"THTdiv\": THTdiv_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTdiveddy\": THTdiveddy_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTdiveddy_cyclones\": THTdiveddy_cyclones_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTdiveddy_anticyclones\": THTdiveddy_anticyclones_season.expand_dims({\"time\": savetime}),\n",
    "            \"THTdivtrack\": THTdivtrack_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTdivtrack_cyclones\": THTdivtrack_cyclones_season.expand_dims({\"time\": savetime}), \n",
    "            \"THTdivtrack_anticyclones\": THTdivtrack_anticyclones_season.expand_dims({\"time\": savetime})\n",
    "            })\n",
    "        Ts_season.to_netcdf(eddypath + 'THTs' + mldname + '.' + s + '.' + str(y1) + '0101_' + str(y2) + '1230.nc')\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322dab3-5ead-4457-a575-6f2b51b91051",
   "metadata": {},
   "source": [
    "And finally we loop over the 10 decades and apply the functions we defined before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dafcca-35cc-453e-a567-5d8955581cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(201, 300, 10)\n",
    "b = np.arange(210, 301, 10)\n",
    "for yy1, yy2 in zip(a, b):\n",
    "    y1 = f\"{yy1:04}\"\n",
    "    y2 = f\"{yy2:04}\"\n",
    "    print(y1,\"to\",y2)\n",
    "    # load and prepare data for current decade\n",
    "    data, THTdiv, em, em_cycl, em_anti, tm, tm_cycl, tm_anti = open_data(path, eddypath, y1, y2)\n",
    "    data, T, V, shelf, rho, Cp, f0, beta, Cd, f, savetime, depMask, nBotDep, grid = param_and_prepare(data, THTdiv)\n",
    "    # calculate full depth, full year heat transports\n",
    "    calc_hts(data, THTdiv, T, V, shelf, em, em_cycl, em_anti, tm, tm_cycl, tm_anti, nBotDep, grid,\n",
    "             False, None, None, None, \"\")\n",
    "    # now include the mixed layer mask\n",
    "    # MLD mask is the mean winter (JJA) mixed layer \n",
    "    nomld = shelf.where(data.Z < grid.interp(-data.MXLDEPTH.groupby(\"time.season\").mean(\"time\").sel(season=\"JJA\"), \n",
    "                                             \"Y\", boundary=\"extend\"), other=0)\n",
    "    nomldm = nomld\n",
    "    nomlds = nomld\n",
    "    calc_hts(data, THTdiv, T, V, shelf, em, em_cycl, em_anti, tm, tm_cycl, tm_anti, nBotDep, grid,\n",
    "             True, nomld, nomldm, nomlds, \"NoML\")\n",
    "    mld = (V / V).isel(time=0).where(data.Z >= grid.interp(-data.MXLDEPTH.groupby(\"time.season\").mean(\"time\").sel(season=\"JJA\"), \n",
    "                                              \"Y\", boundary=\"extend\"), other=0)\n",
    "    mldm = mld\n",
    "    mlds = mld\n",
    "    calc_hts(data, THTdiv, T, V, shelf, em, em_cycl, em_anti, tm, tm_cycl, tm_anti, nBotDep, grid,\n",
    "             True, mld, mldm, mlds, \"ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930c62c-8f45-40cf-9504-ff663e80cb80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python eddytools",
   "language": "python",
   "name": "py_eddytools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
